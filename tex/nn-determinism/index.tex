\documentclass[final]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{ulem}
\usepackage{float}
\usepackage{lastpage}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=black
}

\begin{document}

    \title{Fundamental Determinism of Neural Networks and Security}
    \author{Cherkas Ruslan}
    \date{2025-12-23}


    \begin{small}
        \begingroup
        \renewcommand{\baselinestretch}{0.8}

        \renewcommand{\contentsname}{Contents}
        \maketitle
        \tableofcontents

        \section*{Notes}
            \begin{enumerate}
                \item 10.5281/zenodo.18038515
                \url{https://doi.org/10.5281/zenodo.18038515}

                \item Original text [en]
                \url{https://github.com/johnthesmith/scraps/blob/main/en/nn-determinism.md}

                \item The article is published under the license:
                \url{https://creativecommons.org/licenses/by-sa/4.0/} CC BY-SA 4.0
            \end{enumerate}
        \endgroup
    \end{small}
   
    \begin{abstract}
        The article proposes viewing AI safety as an engineering process, 
        emphasizing that neural networks should be deterministic under fixed 
        inputs, weights, and execution conditions. Any observed variability is 
        considered a defect in hardware, software, or algorithms, highlighting 
        the need for strict control and reproducibility in AI systems.
    \end{abstract}

    \section{Introduction}

        \noindent
        By the nature of my work, I often encounter claims about the 
        \textbf{fundamental non-determinism} of neural networks given identical input 
        data.

        \noindent
        It can be stated that for any neural network of the form
        \[
        r = f(a, w)
        \]
        of any type (LLM, convolutional, etc.), with fixed arguments \(a\), fixed
        weights \(w\), and identical execution conditions, the result \(r\) is
        deterministic. Any variation in the result without changes to \(a\), \(w\),
        or execution conditions is an implementation error.

        \noindent
        Separately, recurrent networks of the form
        \[
        r_1 = f(a, w, r_0)
        \]
        must be noted, where the result \(r_1\) explicitly depends on the previous
        state \(r_0\). However, even in this case, all else being equal (including
        \(r_0\)), the result \(r_1\) must be deterministic.

        \noindent
        This assertion also applies to the training process
        \[
        w = l(a, p)
        \]
        where \(w\) are the resulting weights, \(a\) the training arguments, and
        \(p\) the training data.

        \noindent
        Let us examine the common objections typically cited as causes of
        non-determinism.

    \section{Non-determinism of Input Data}

        \noindent
        \textbf{Objection}: the network is fundamentally non-deterministic due to 
        random values during initialization.

        \noindent
        \textbf{Refutation}: all numerical values, including weights, are fixed; there 
        is no internal randomness without changing the arguments. Any violation 
        of identical arguments for repeated computations is an implementation 
        error.

    \subsection{Hardware Non-determinism}

        \noindent
        \textbf{Objection}: results may vary due to hardware non-determinism.

        \noindent
        \textbf{Refutation}: with identical hardware and precision, hardware 
        non-determinism is excluded. The presence of non-determinism indicates 
        hardware faults and invalidates the result.

    \subsection{Software Non-determinism}

        \noindent
        \textbf{Objection}: results may vary due to software implementation details, 
        including library versions, internal caching, or optimizations.

        \noindent
        \textbf{Refutation}: with an identical software environment and library 
        versions, the result must be deterministic; otherwise, it must be 
        classified as a software defect.

    \subsection{Algorithmic Non-determinism}

        \noindent
        \textbf{Objection}: results may vary due to algorithmic non-determinism, 
        including parallel execution, race conditions, or operation ordering.


        \noindent
        \textbf{Refutation}: non-determinism without changing inputs equals a bug. An
        algorithm must produce a single result for fixed data and a fixed
        sequence of steps. If the order “floats,” producing for
        \[
        x + y + z
        \]
        the result
        \[
        (x + y) + z \neq x + (y + z),
        \]
        this must be considered an incorrect implementation.

    \subsection{External Factors}

        \noindent
        \textbf{Objection}: a network may produce different results due to fuzzy 
        operations (quantum factors) or external world parameters 
        (measurements).

        \noindent
        \textbf{Refutation}: any external-world parameter, including quantum effects or 
        other factors, can affect the result only via explicit model arguments. 
        If a factor directly influences the result without being explicitly 
        included in the arguments, this is an implementation error.

    \section{Actions}

        I will outline the actions that should be taken when non-determinism is 
        detected in a model.

    \subsection{Acknowledging the Problem}

        The presence of non-determinism is an indicator of a problem. The problem must
        be acknowledged and explicitly stated.

    \subsection{Problem Analysis}

        When non-determinism is detected, its source must be identified and classified
        into one of the categories discussed above:

        \begin{enumerate}

            \item Arguments — randomness in inputs or weights, untracked state 
            changes, etc.
            
            \item Hardware non-determinism — differing operation order, hardware 
            faults.
            
            \item Software non-determinism — library versions, optimizations.
            
            \item Algorithmic non-determinism — implementation errors, 
            parallelism, violation of execution order.

        \end{enumerate}

    \subsection{Problem Resolution}

        Based on the analysis, measures must be taken to eliminate the problem, 
        with a demonstration of deterministic results.

    \subsection{Intentional Introduction of Non-determinism}

        I allow cases where “randomness” is required. However, it must be 
        explicitly included in the model arguments, with the ability to 
        repeatedly test identical arguments. The model must still demonstrate 
        determinism.

    \section{Conclusions}

        Based on the above, I propose the following conclusions:

        \begin{enumerate}

            \item Claims about the \textbf{fundamental} non-determinism of 
            mathematical neural networks indicate either a lack of understanding 
            of what is happening or an attempt to conceal errors at the 
            hardware, software, or algorithmic levels.

            \item The inability of a neural network to demonstrate deterministic 
            results should be considered a dangerous problem due to 
            unpredictability, especially in business-critical and 
            mission-critical systems.

            \item The use of any models in business-critical and 
            mission-critical systems must be based on an understanding of the 
            sources of non-determinism, their control, and their elimination.

            \item Implementing a deterministic execution pipeline for neural 
            networks should be a conscious, high-priority goal.
        \end{enumerate}

    \section{Final Statement}

        In summary: discussions about the safety of neural network development, 
        and AI in general, must be grounded in achieving guaranteed 
        repeatability and verifiability of AI results. This is achievable.




    \vspace{1em}
    \textbf{Remark.} Non-determinism in the execution pipeline should be treated as
    an implementation defect rather than a feature.



    \begin{thebibliography}{9}

        \bibitem{henderson_deep_rl_that_matters_2018}
        Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D.,
        \textit{Deep Reinforcement Learning That Matters},
        Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, pp. 3247–3254, 2018.
        URL: \url{https://ojs.aaai.org/index.php/AAAI/article/view/11694}

        \bibitem{pineau_improving_reproducibility_2021}
        Pineau, J., Sinha, K., Fried, G., Ke, R. N., Larochelle, H.,
        \textit{Improving Reproducibility in Machine Learning Research},
        Journal of Machine Learning Research, vol. 22, no. 164, pp. 1–20, 2021.
        URL: \url{https://jmlr.org/papers/v22/20-303.html}

    \end{thebibliography}

\end{document}
