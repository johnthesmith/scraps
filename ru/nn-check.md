# Вступление

Считаю, что вопросы надежности и безопасности искусственного интеллекта 
определяющий для его применения в ответственных областях. Необходимо различать 
два независимых аспекта:

1. Повторяемость. Как показано в 
[предыдущей статье](https://github.com/johnthesmith/scraps/blob/main/ru/nn-determinism.md), 
любая нейросеть должна быть детерминирована: при фиксированных входных данных, 
весах и условиях исполнения результат обязан быть неизменным. Недетерминизм — 
это всегда ошибка реализации (баг), недопустимый в промышленных системах уровней 
`mission-critical` и `business-critical`.
2. Проверяемость. Ответ нейросети должен соответствовать наблюдаемой и 
проверяемой истине. Следовательно, результат должен быть проверяем.

Далее рассматриваем способы проверки результата.



# Способы проверки корректности результатов

Перичислю методы верификации по источнику истины:

1. [Человек](#человек)
2. [Эксперимент](#эксперимент)
3. [Формальная спецификация или конечный алгоритм](#формальная-спецификация-или-конечный-алгоритм)
4. [Нейросеть](#нейросеть)
5. [Смешанный подход](#смешанный-подход)



## Человек

Человек выступает прямым источником истины. Эксперт оценивает результат, исходя 
из собственного знания о том, каким должен быть правильный ответ. Для снижения 
субъективности применяется коллективная оценка. 

Да, человек может ошибаться. Однако человечество дожило до текущего момента, 
опираясь именно на этот способ оценки — значит, как минимум он эволюционно 
оправдан.



## Эксперимент

Эксперимент — проверка нейросети наблюдаемой реальностью. Результат 
сверяется с тем, что происходит в физическом мире. 

На мой взгляд это наиболее достоверный способ верификации.

Ограничение: эксперимент часто невозможен до начала реальной эксплуатации либо 
слишком дорог, опасен или недоступен в реалтайме.



## Формальная спецификация или конечный алогритм

Этот способ опирается на заранее известное формализованное знание о правильном 
результате, наличие функциональной зависимости или статистики реального мира.

Часто мы не можем заранее вычислить правильный результат. Но когда эвристический 
ответ уже получен от нейросети, проверить его конечным алгоритмом оказывается 
вполне возможно.

Преимущество подхода — объективность и воспроизводимость. 

Ограничение — применим только там, где правильный ответ поддается формализации.



## Нейросеть

Переходим к недостоверным способам проверки.

Использование другой нейросети в качестве эталона — распространенный, но 
ошибочный подход. Он основан на ложном предположении, что если одна сеть 
достаточно хороша, то она может служить мерилом для другой.

"Эталонная" модель: Берется нейросеть, которую считают более точной (например, 
большего размера или обученная дольше), и ее ответы принимаются за правильные 
при проверке другой сети.

Ансамблевые методы. Усредняются ответы нескольких сетей, и полученное среднее 
считается эталоном для проверки каждой отдельной сети.

Кросс-валидация между архитектурами. Сравниваются ответы сетей разной 
архитектуры, и совпадение интерпретируется как подтверждение правильности.

Проблема всех этих методов в том, что они не дают выхода к истине — только 
сравнивают разные аппроксимации между собой.



## О смешанном подходе

Смешанный подход: результаты алгоритма или человека используются для обучения 
нейросети, которая затем проверяет другую нейросеть.

На первый взгляд допустимо — эталон был внешним. Но ошибка лишь скрыта, а не 
устранена. 

Причины:

1. Потери независимости. Обученная на эталоне нейросеть — это лишь 
аппроксимация, а не сам эталон. Гарантии исходного источника утрачены.
2. Невозможность верификации границ. Алгоритм или человек давали ответы на 
конечном множестве. Нейросеть экстраполирует на бесконечное пространство, и 
неизвестно, где ее ответы расходятся с исходным эталоном.
3. Циркулярность. Проверка одной аппроксимации через другую — потеря прямой 
связи с первоисточником истины.



# Доказательство некорректности проверки через обученную сеть

1. Пусть `T` — истина, неизвестная нейросети напрямую, тк не имеет компактного 
описания (иначе задача решалась бы классическими методами без применения 
нейросетей).

2. Пусть `N1 = learning( T, D1 )` и `N2 = learning( T, D2 )`, где:
    1. `D1` и `D2` — конечные выборки из `T`, 
    0. `N1` и `N2` — результат обучения двух сетей.

причем:

```
D1 ≠ D2, D1 ∪ D2 ≠ T
```

3. Каждая нейросеть видела только свой кусок истины и никогда не видела всю `T` 
целиком. те процедура обучения в общем случае не гарантирует:
```
∀x: N1( x ) = T( x )
∀x: N2( x ) = T( x )
```

4. Рассмотрим проверку `N1` через `N2` как требование:

```
∀x: N1( x ) = N2( x )
```

5. Возьмем `x* ∉ D1 ∪ D2`, те `x*` не видела ни первая ни вторая сеть в обучающих 
сэмплах. Для этой точки обе сети дают ответ, не подтвержденный истиной `T`.

При этом `N1( x )` может равняться `N2( x )`, но гарантий, что это значение 
равно `T(x*)`, нет — `T` для этой точки неизвестна.

Следовательно:
```
(∀x: N1( x ) = N2( x )) ≠> (∀x: N1( x ) = T( x ))
```

Совпадение ответов двух нейросетей не является свидетельством истинности. 
Никакая нейросеть не может служить эталоном для другой.

Даже если `N1( x* ) = N2( x* )`, это может быть совпадение «по случайности» или 
системная ошибка обобщения, а не подтверждение истинного значения.



# Вывод

1. Использовать одну нейросеть для проверки другой — критически небезопасно.

2. Совпадение ответов не дает никаких гарантий корректности. Обе могут одинаково 
ошибаться в точках, не виденных ни одной из них при обучении. А где именно 
пролегают эти точки и какова истина — неизвестно.

3. Как показано в [предыдущей статье](https://github.com/johnthesmith/scraps/blob/main/ru/nn-determinism.md), 
детерминированность — необходимое условие надежности, но недостаточное. Для 
`mission-critical` и `business-critical` систем помимо повторяемости результатов 
требуется их проверяемость.

4. Единственный допустимый эталон — **внешний и независимый**: человек, 
эксперимент, формальная спецификация (производная от человека) или конечный 
алгоритм. Всё остальное — создание иллюзии контроля вместо реальной верификации.

---

PS: я признаю, что проверка одной нейросети другой нейросетью, это компромис 
развития, однако утверждаю, что на большом отрезке времени при высокой сложности 
данная практика приведет либо к медленной деградации либо к внезапной катострофе 
эксплуатируемой системы.

Дыры в знаниях `x* ∉ ( D1 ∪ D2 )` никуда не исчезают — они лишь маскируются 
взаимным подтверждением ошибок, а их последствия накапливаются.
