# Верификация нейросети как условие безопасной эксплуатации

Считаю, что вопросы надежности и безопасности искусственного интеллекта 
определяющий для его применения в ответственных областях. Необходимо различать 
два независимых аспекта:

1. Повторяемость результата. Как показано в 
[предыдущей статье](https://github.com/johnthesmith/scraps/blob/main/ru/nn-determinism.md), 
любая нейросеть должна быть детерминирована: при фиксированных входных данных, 
весах и условиях исполнения результат обязан быть неизменным. Недетерминизм — 
это всегда ошибка реализации (баг), недопустимый в промышленных системах уровней 
`mission-critical` и `business-critical`.
2. Проверяемость результата. Ответ нейросети должен соответствовать наблюдаемой 
и проверяемой истине. Следовательно, результат должен быть проверяем.

Далее рассматриваем способы проверки результата.



# Методы верификаци

Перечислю методы верификации по источнику истины:

1. [Человек](#человек)
2. [Эксперимент](#эксперимент)
3. [Формальная спецификация или конечный алгоритм](#формальная-спецификация-или-конечный-алгоритм)
4. [Нейросеть](#нейросеть)
5. [Смешанный подход](#смешанный-подход)



## Человек

Человек выступает прямым источником истины. Эксперт оценивает результат, исходя 
из собственного знания о том, каким должен быть правильный ответ. Для снижения 
субъективности применяется коллективная оценка. 

Да, человек может ошибаться. Однако человечество дожило до текущего момента, 
опираясь именно свой опыт — следовательно, он эволюционно оправдан.

Кроме того к человеку применимо понятие возложения ответственности, что 
позволяет оперировать рисками.

К сожалению, верификация ответа человеком затруднительна в runtime процессах,
требующих высокой скорости реакции. Следующий метод частично позволяет решить
данную проблему.



## Формальная спецификация или конечный алогритм

Этот способ опирается на заранее известное формализованное полное знание о 
правильном результате, наличие функциональной зависимости или статистики 
реального мира (приближающей к истине).

Часто мы не можем заранее вычислить правильный результат. Но когда эвристический 
ответ уже получен от нейросети, проверить его конечным алгоритмом оказывается 
вполне возможно. При этом, 
[детерминированная](https://github.com/johnthesmith/scraps/blob/main/ru/nn-determinism.md) 
нейросеть безусловно является конечным алгоритмом но она по определению не 
обладает всей полнотой истины а потому не годится для проверки, что показано 
[ниже](#доказательство-некорректности-проверки-через-обученную-сеть).

Преимущество подхода — объективность и воспроизводимость. 

Ограничение — метод применим только там, где правильный ответ поддается 
формализации.



## Эксперимент

Эксперимент — проверка нейросети наблюдаемой реальностью. Результат сверяется с 
тем, что происходит в физическом мире. При этом, в задачу для нейросети может 
быть включено требование предоставления условий и параметров для проведения 
проверочного эксперимента. Оценка же результатов может проводиться исключительно 
выше перечисленными методами.

На мой взгляд это наиболее полный и достоверный способ верификации.

Ограничение: эксперимент часто невозможен до начала реальной эксплуатации либо 
слишком дорог, опасен или недоступен в realtime.



## Нейросеть

Переходим к недостоверным способам проверки.

Использование другой нейросети в качестве эталона — распространенный, но 
ошибочный подход. Он основан на ложном предположении, что если одна сеть 
достаточно хороша, то она может служить мерилом для другой.

Перечислю попытки валидации с которыми сталкивался:

"Эталонная" модель: Берется нейросеть, которую считают более точной (например, 
большего размера или обученная дольше), и на основе ее ответов выполняется
оценка результатов проверяемой сети.

Ансамблевые методы. Усредняются ответы нескольких сетей, и полученное среднее 
считается эталоном для проверки каждой отдельной сети.

Кросс-валидация между архитектурами. Сравниваются ответы сетей разной 
архитектуры, и совпадение интерпретируется как подтверждение правильности.

Проблема всех этих методов в том, что они не дают выхода к истине — только 
сравнивают разные аппроксимации между собой. Методы порождают вопросы:

1. "нужно ли для проверки второй нерости использовать третью?"
2. "почему бы не использовать проверяющую сеть в качестве основной?"
3. и прочее ...



## О смешанном подходе

Смешанный подход: результаты алгоритма или человека используются для обучения 
нейросети, которая затем проверяет другую нейросеть.

На первый взгляд допустимо — эталон был внешним. Но ошибка лишь скрыта, а не 
устранена. 

### Причины

1. Потери независимости. Обученная на эталоне нейросеть — это лишь 
аппроксимация, а не сам эталон. Гарантии исходного источника утрачены.
2. Невозможность верификации границ. Алгоритм или человек давали ответы на 
конечном множестве. Нейросеть экстраполирует на бесконечное пространство, и 
неизвестно, где ее ответы расходятся с исходным эталоном.
3. Циркулярность. Проверка одной аппроксимации через другую — потеря прямой 
связи с первоисточником истины.



# Доказательство некорректности проверки через обученную сеть

Приведу доказательство несостоятельности двух выше описанных методов:

1. Пусть `T` — истина, неизвестная нейросети напрямую, тк не имеет компактного 
описания (иначе задача решалась бы классическими методами без применения 
нейросетей).

2. Пусть `N1 = learning( T, D1 )` и `N2 = learning( T, D2 )`, где:
    1. `D1` и `D2` — конечные выборки из `T`, 
    0. `N1` и `N2` — результат обучения двух сетей.

причем:

```
D1 ≠ D2, D1 ∪ D2 ≠ T
```

Те каждая нейросеть видела только свой кусок истины и никогда не видела всю `T` 
целиком. 

Следлвательно процедура обучения в общем случае не гарантирует:
```
∀x: N1( x ) = T( x )
∀x: N2( x ) = T( x )
```

3. Рассмотрим проверку `N1` через `N2` как требование:

```
∀x: N1( x ) = N2( x )
```

4. Возьмем `x' ∉ D1 ∪ D2`, те `x'` не видела ни первая ни вторая сеть в обучающих 
сэмплах. Для этой точки обе сети дают ответ, не подтвержденный истиной `T`.

При этом `N1( x' )` может равняться `N2( x' )`, но гарантий, что это значение 
равно `T(x*)`, нет — `T` для этой точки неизвестна.

Следовательно:
```
(∀x: N1( x ) = N2( x )) ≠> (∀x: N1( x ) = T( x ))
```

Совпадение ответов двух нейросетей не является свидетельством истинности. 
Никакая нейросеть не может служить эталоном для другой.

Даже если `N1( x' ) = N2( x' )`, это может быть совпадение «по случайности» или 
системная ошибка обобщения, а не подтверждение истинного значения.



# Вывод

1. Использование одной нейросети для проверки другой не дает достоверных 
результатов. Совпадение ответов не дает никаких гарантий корректности. Обе могут 
одинаково ошибаться в точках, не представленных ни одной из них при обучении. А где 
именно пролегают эти точки и какова истина — неизвестно.

2. Для `mission-critical` и `business-critical` систем помимо 
[повторяемости](https://github.com/johnthesmith/scraps/blob/main/ru/nn-determinism.md), 
результатов требуется их проверяемость, что достижимо только при участии 
внешнего и независимого эталона ( человек - принимающий ответсвенность ошибки, 
эксперимент как объективное знание о мире, конечный алгоритм вычисляющий истину 
для всех аргументов, etc).



# Основное

Проверка одной нейросети через другую — создание иллюзии контроля вместо 
реальной верификации, и несет риски. Дыры в знаниях `x* ∉ ( D1 ∪ D2 )` никуда не 
исчезают — они лишь маскируются взаимным подтверждением ошибок, а их последствия 
накапливаются.


---

PS: я признаю, что нейросетьевая кроссвалидация, это компромис развития, однако 
утверждаю, что на большом отрезке времени данная практика приведет либо к 
медленной деградации либо к внезапной катастрофической ошибке в управляемой или
контролируемой системе.
